{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "096fcff2-6dd2-42e3-9325-0566ecc1d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import logical_and as land\n",
    "from numpy import logical_or as lor\n",
    "from numpy import invert as lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90dc0a2c-c2f5-4096-b367-ad491ff2cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theta(mu, cov, rho):\n",
    "    '''\n",
    "    Returns explicit threshold theta for a given percentage rho of anomalies in \n",
    "    data distributed as a Gaussian with mean mu and covariance matrix cov. \n",
    "    \n",
    "    Parameters\n",
    "        mu    mean of Gaussian distribution\n",
    "        cov   covariance matrix of Gaussian distribution\n",
    "        rho   percentage of anomalies, which must be between 0 and 100 inclusive\n",
    "    '''\n",
    "    # generate random variables (data)\n",
    "    X = multivariate_normal.rvs(mean=mu, cov=cov, size=5000000)\n",
    "    # center data (normalize) (for x_i - mu)\n",
    "    Z = X - mu\n",
    "    # calculate the mahalanobis distance\n",
    "    # d2M (xi, ˆμ) = (xi − ˆμ)T ˆΣ−1(xi − ˆμ)\n",
    "    d = np.sqrt(np.sum(Z.dot(inv(cov)) * Z, axis=1))\n",
    "    # thetha = \n",
    "    return np.percentile(d, 100-rho) \n",
    "\n",
    "# get_theta([0, 0], [[1, 0], [0, 1]], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f8afa7-8011-44c1-90c5-d79807ecaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# styling and fig siz\n",
    "plt.style.use('seaborn-dark')\n",
    "plt.rcParams['figure.figsize']= 16, 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235e3ee2-8865-44f1-b17c-d11d203d6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for plotting etc\n",
    "\n",
    "def gen_data(mu, cov, n=1000):\n",
    "    '''\n",
    "    generate bivariate gaussian data\n",
    "    \n",
    "    mu mean of the gaussian distribution\n",
    "    cov covariance matrix\n",
    "    n size (number of points)\n",
    "    '''\n",
    "    return multivariate_normal.rvs(cov=cov, mean=mu, size=n)\n",
    "\n",
    "def plt_points(data):\n",
    "    '''\n",
    "    plot bivariate gaussian data as points\n",
    "    '''\n",
    "    # Plotting the generated samples\n",
    "    plt.plot(data[:,0], data[:,1], 'o', c='lime',\n",
    "             markeredgewidth = 0.5,\n",
    "             markeredgecolor = 'black')\n",
    "    # plt.title('covariance of distribution')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.axis('equal')\n",
    "     \n",
    "    plt.show()\n",
    "    \n",
    "def plt_pdf(mu, cov):\n",
    "    '''\n",
    "    plot the density function from a bivariate gaussian distribution\n",
    "    \n",
    "    mu mean\n",
    "    cov covariance matrix\n",
    "    '''\n",
    "    distr = multivariate_normal(cov=cov, mean=mu)\n",
    "     \n",
    "    # Generating a meshgrid complacent with\n",
    "    # the 3-sigma boundary\n",
    "    mean_1, mean_2 = mean[0], mean[1]\n",
    "    sigma_1, sigma_2 = cov[0,0], cov[1,1]\n",
    "     \n",
    "    x = np.linspace(-3*sigma_1, 3*sigma_1, num=100)\n",
    "    y = np.linspace(-3*sigma_2, 3*sigma_2, num=100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "     \n",
    "    # Generating the density function\n",
    "    # for each point in the meshgrid\n",
    "    pdf = np.zeros(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            pdf[i,j] = distr.pdf([X[i,j], Y[i,j]])\n",
    "     \n",
    "    # Plotting the density function values\n",
    "    ax = plt.figure().add_subplot(111, projection = '3d')\n",
    "    ax.plot_surface(X, Y, pdf, cmap = 'viridis')\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    ax.axes.zaxis.set_ticks([])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402a9923-6d8c-4b78-9628-8b5ca9848b0a",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "1. Sample a data set D of size n from N (x; μ, Σ). Fix a percentage ρ.\n",
    "2. Use the function get_theta(mu, cov, rho) provided by the notebook to\n",
    "obtain an explicit threshold θ given the percentage ρ. Note that θ is part\n",
    "of the ground-truth and therefore considered as unknown.\n",
    "3. Determine the true anomalies of D. For this, use the explicit threshold θ\n",
    "together with the Mahalanobis distance d∗\n",
    "M defined by the true μ and Σ.\n",
    "4. Use the data D to estimate μ and Σ. Construct the Mahalanobis distance\n",
    "dM defined by the estimates ˆμ and ˆΣ.\n",
    "5. Predict the anomalies of D using the Mahalanobis distance dM and Eu-\n",
    "clidean distance dE . Anomalies are the ρ percent points xi ∈ D farthest\n",
    "from ˆμ (do not use θ). Assess precision and recall of both detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacb0cf2-3dc4-4581-9214-19549490b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixate groundtruth mean and covariance matrix for the bivariate gaussian distribution\n",
    "# '_T' nominator stands for groundtruth variable\n",
    "# '_E' nominator stands for estimated variable\n",
    "mu_T = np.array([0, 0])  # mean at (0, 0)\n",
    "sigma_T = np.array([[1, 0], [0, 1]])  # covariance matrix\n",
    "rho = 5  # preset percentage of outliers\n",
    "size = 5000  # number of data points    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ab0970-b978-49bd-85ed-f02378346633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. generate dataset (RandomVariableS)\n",
    "D = multivariate_normal.rvs(mean=mu_T, cov=sigma_T, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "616a626d-850d-45df-aa59-6736938d23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. use get_theta to get the 'groundtruth' explicit treshold theta\n",
    "theta = get_theta(mu_T, sigma_T, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7907422c-8d51-4480-8fe2-db5927afe739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. determine subset of true anomalies of dataset D\n",
    "# start by calculating the mahalanobis distance of each point from the mean\n",
    "Z_T = D - mu_T\n",
    "d_m_T = np.sqrt(np.sum(Z_T.dot(inv(sigma_T)) * Z_T, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dca05dd3-152c-4368-8670-6945c082bf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0496"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out values (indices) over the groundtruth threshold theta (True / False array)\n",
    "I_m_T = d_m_T > theta  # indices of true anomalies with mahalanobis distance\n",
    "# print percentage of as true determined inices\n",
    "I_m_T.sum() / len(I_m_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d25774-aba0-4705-92da-b18c9ea52ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use the data D to estimate mu and sigma\n",
    "mu_E = D.mean(axis=0)\n",
    "sigma_E = np.cov(D.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24398f13-3ff0-4da7-ad7d-a25b435d7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Mahalanobis distance d_m_E defined by the estimates mu_E and sigma_E\n",
    "Z_E = D - mu_E\n",
    "d_m_E = np.sqrt(np.sum(Z_E.dot(inv(sigma_E)) * Z_E, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aff27058-3bf3-47ae-9b68-6d86bc690825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct euklidian distance d_e_E in the same manner (with mu_E and sigma_E)\n",
    "d_e_E = np.sqrt(np.sum(Z_E ** 2, axis=1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce279073-e727-4830-a752-db476682dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.79223118 0.25082244 1.56623271 ... 0.34458559 0.50931603 0.52295871]\n",
      "[1.79550192 0.24541798 1.56855232 ... 0.33734012 0.50706179 0.52004925]\n"
     ]
    }
   ],
   "source": [
    "print(d_e_E)\n",
    "print(d_m_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dcd1ca3-ad4a-4391-a7ac-6805d43e814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. predict anomalies with estimated eucilidian (d_e_E) and mahalanobis distance (d_m_E)\n",
    "# create list of indices (True / False array) (on axis 0 of dataset)\n",
    "# estimated thresholds (eta) are rho percent points with the farthest distance from mu_E\n",
    "eta_m = np.percentile(d_m_E, 100-rho)\n",
    "eta_e = np.percentile(d_e_E, 100-rho) \n",
    "\n",
    "I_m_E = d_m_E > eta_m\n",
    "I_e_E = d_e_E > eta_e \n",
    "\n",
    "assert len(I_m_E[I_m_E]) / len(I_m_E) == rho * .01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4586bdb-70ae-44d8-a45a-04156d699d39",
   "metadata": {},
   "source": [
    "# Precision & Recall:\n",
    "Performance metrics for anomaly detection are precision `tp/(tp + fp)` and recall `tp/(tp + fn)`, where\n",
    "- tp is the number of true positives, that is the number of points that are\n",
    "correctly predicted as anomalies\n",
    "- fp is the number of false positives, that is the number of normal points\n",
    "that are falsely predicted as anomalies\n",
    "- fn is the number of false negatives, that is the number of anomalies that\n",
    "are falsely predicted as normal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2237763b-eb8a-4d21-a6cb-43b754a756d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4752"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison:\n",
    "# Assess precision and recall of both detectors. (5)\n",
    "# calculate tp, fp and fn for euklidian distance and for mahalanobis distance\n",
    "\n",
    "# np.logical_and(I_m_T, I_m_E) [here: land] creates a logical AND mask over the two boolean arrays etc.\n",
    "# (I_m_T * I_m_E)\n",
    "tp_m = land(I_m_T, I_m_E).sum()\n",
    "tp_e = land(I_m_T, I_e_E).sum()\n",
    "\n",
    "fp_m = lin(I_m_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538059f0-3f81-4248-9b72-0f1432601063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
